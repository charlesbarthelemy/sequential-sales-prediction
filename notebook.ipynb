{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0) Imports & Setup\n",
    "###############################################################################\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Class weights (Tensor of size V)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) Global Parameters\n",
    "###############################################################################\n",
    "PATH_CSV = '*****************'\n",
    "\n",
    "MANDATORY_FEATURES = ['*****************', '*****************']  # ➔ Mandatory Featrures\n",
    "\n",
    "CAT_FEATURES = ['*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************']         # ➔ Categorical Features\n",
    "\n",
    "NUM_FEATURES = ['*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************']         # ➔ Numerical Features\n",
    "\n",
    "TARGET       = ['*****************']\n",
    "\n",
    "NOT_CONSIDERED = ['*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "                '*****************', '*****************', '*****************']\n",
    "\n",
    "MIN_SEQ = 3         # Minimum length of a sequence\n",
    "SEQ_LEN = 15        # Length of a sequence (for padding)\n",
    "MAX_SEQ = 20        # Maximum length of a sequence\n",
    "\n",
    "SPLIT = 0.9         # Ratio train / test\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 32\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "\n",
    "EARLY_STOPPING = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) Loading and Preprocessing\n",
    "###############################################################################\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(PATH_CSV, usecols=CAT_FEATURES+NUM_FEATURES+MANDATORY_FEATURES+TARGET)\n",
    "\n",
    "# Delete accessories\n",
    "df = df[df['*****************']!='*****************']\n",
    "\n",
    "# Delete not considered machines\n",
    "other = ['*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "         '*****************', '*****************', '*****************', '*****************', '*****************', '*****************',\n",
    "         '*****************', '*****************', '*****************']\n",
    "df = df[~df['*****************'].isin(other)]\n",
    "\n",
    "\n",
    "# Replace rare machines with 'Other'\n",
    "min_freq = 100\n",
    "code_counts = df['*****************'].value_counts()\n",
    "rare_codes = code_counts[code_counts < min_freq].index\n",
    "df['*****************'] = df['*****************'].apply(lambda x: x if x not in rare_codes else 'Other')\n",
    "\n",
    "# Delete clients with less than MIN_SEQ sequences\n",
    "df['Count'] = df.groupby('*****************')['*****************'].transform('count')\n",
    "df = df[df['Count'] >= MIN_SEQ].drop(columns=['Count'])\n",
    "\n",
    "# Date conversion and sorting\n",
    "df['*****************'] = pd.to_datetime(df['*****************'])\n",
    "df = df.sort_values(['*****************', '*****************'])\n",
    "\n",
    "# ----------------PREPROCESSING----------------\n",
    "if '*****************' in df.columns:\n",
    "    # Create an age_machine feature\n",
    "    df['*****************'] = pd.to_datetime(df['*****************'], errors='coerce')\n",
    "    df['*****************'] = (df['*****************'] - df['*****************']).dt.days / 365.25\n",
    "    df['*****************'] = df['*****************'].fillna(df['*****************'].median())\n",
    "    NUM_FEATURES.append('*****************')\n",
    "    NUM_FEATURES.remove('*****************')\n",
    "\n",
    "if '*****************' in df.columns:\n",
    "    df['*****************'] = df['*****************'].isnull().astype(int)\n",
    "    NUM_FEATURES.append('*****************')\n",
    "\n",
    "l = ['*****************', '*****************', '*****************', '*****************']\n",
    "\n",
    "for col in l:\n",
    "    if col in df.columns:\n",
    "        df[col] = np.log1p(df[col].fillna(0))\n",
    "\n",
    "if '*****************' in df.columns:\n",
    "    df['*****************'] = np.log1p(df['*****************'].fillna(0))\n",
    "    NUM_FEATURES += ['*****************']\n",
    "if '*****************' in df.columns:\n",
    "    df['*****************'] = np.log1p(df['*****************'].fillna(0))\n",
    "    NUM_FEATURES += ['*****************']\n",
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage Label\n",
    "LabelToIdx = {}\n",
    "for feat in CAT_FEATURES + TARGET:\n",
    "    enc = LabelEncoder()\n",
    "    df[f\"{feat}_enc\"] = enc.fit_transform(df[feat]) + 1\n",
    "    LabelToIdx[feat] = {cls: idx+1 for idx, cls in enumerate(enc.classes_)}\n",
    "    df.drop(columns=[feat], inplace=True)\n",
    "\n",
    "vocab_sizes = {\n",
    "    feat: len(LabelToIdx[feat]) + 1  # +1 for padding\n",
    "    for feat in CAT_FEATURES + TARGET\n",
    "}\n",
    "\n",
    "emb_dims = {}\n",
    "for feature in CAT_FEATURES + TARGET:\n",
    "    if vocab_sizes[feature] < 100:\n",
    "        emb_dims[feature] = 8\n",
    "    elif vocab_sizes[feature] < 1000:\n",
    "        emb_dims[feature] = 32\n",
    "    else:\n",
    "        emb_dims[feature] = 64\n",
    "\n",
    "target_vocab_size = vocab_sizes[TARGET[0]]\n",
    "\n",
    "TARGET = [TARGET[0] + '_enc']\n",
    "\n",
    "#\t•\tSmall vocabularies (fewer than 100) → 4 to 8 dimensions\n",
    "#\t•\tMedium vocabularies (100–1000) → 8 to 32 dimensions\n",
    "#\t•\tVery large vocabularies (over 1000) → 32+ dimensions\n",
    "\n",
    "# Scaling Numerical Features\n",
    "for feat in NUM_FEATURES:\n",
    "    scaler = StandardScaler()\n",
    "    df[f\"{feat}_enc\"] = scaler.fit_transform(df[[feat]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Dataset et DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) Dataset et DataLoader\n",
    "###############################################################################\n",
    "\n",
    "# a) Preparing the DataFrame for LSTM\n",
    "df_lstm = df.copy()\n",
    "df_lstm.sort_values(['*****************', '*****************'], inplace=True)\n",
    "df_lstm = df_lstm.groupby('*****************').agg(list).reset_index()\n",
    "df_lstm.drop(columns=['*****************'], inplace=True)\n",
    "\n",
    "# b) Custom dataset with trimming at MIN_SEQ and padding to MAX_SEQ\n",
    "class MachineDataset(Dataset):\n",
    "    def __init__(self, df_lstm, cat_features, num_features, target_col, max_seq, min_seq):\n",
    "        self.samples = []\n",
    "        self.cat_features = cat_features\n",
    "        self.num_features = num_features\n",
    "        self.target_col = target_col\n",
    "        self.max_seq = max_seq\n",
    "        self.min_seq = min_seq\n",
    "\n",
    "        for _, row in df_lstm.iterrows():\n",
    "            L = len(row[target_col])\n",
    "            for i in range(1, L):\n",
    "                if i < min_seq:\n",
    "                    continue\n",
    "                sample = {}\n",
    "\n",
    "                # Categorical\n",
    "                for feat in self.cat_features:\n",
    "                    full_seq = row[feat + '_enc'][:i]\n",
    "                    sample[feat] = torch.tensor(full_seq[-self.max_seq:], dtype=torch.long)\n",
    "\n",
    "                # Numerical\n",
    "                for feat in self.num_features:\n",
    "                    full_seq = row[feat + '_enc'][:i]\n",
    "                    sample[feat] = torch.tensor(full_seq[-self.max_seq:], dtype=torch.float)\n",
    "\n",
    "                # Target\n",
    "                sample['target'] = torch.tensor(row[target_col][i], dtype=torch.long)\n",
    "                self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        x = {feat: sample[feat] for feat in self.cat_features + self.num_features}\n",
    "        y = sample['target']\n",
    "        return x, y\n",
    "\n",
    "# c) Collate Function - to deal with variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    features_batch = {feat: [] for feat in batch[0][0].keys()}\n",
    "    targets = []\n",
    "\n",
    "    for x, y in batch:\n",
    "        for feat in features_batch:\n",
    "            features_batch[feat].append(x[feat])\n",
    "        targets.append(y)\n",
    "\n",
    "    # Padding\n",
    "    for feat in features_batch:\n",
    "        dtype = features_batch[feat][0].dtype\n",
    "        pad_val = 0 if dtype == torch.long else 0.0\n",
    "        features_batch[feat] = pad_sequence(features_batch[feat], batch_first=True, padding_value=pad_val)\n",
    "\n",
    "    targets = torch.stack(targets)\n",
    "    return features_batch, targets\n",
    "\n",
    "\n",
    "# d) Train/Test Split\n",
    "clients = df_lstm['IdClient'].unique()\n",
    "np.random.shuffle(clients)\n",
    "n_train = int(SPLIT * len(clients))\n",
    "train_clients = set(clients[:n_train])\n",
    "test_clients  = set(clients[n_train:])\n",
    "\n",
    "# e) Creating train and test datasets\n",
    "df_train = df_lstm[df_lstm['IdClient'].isin(train_clients)].reset_index(drop=True)\n",
    "df_test  = df_lstm[df_lstm['IdClient'].isin(test_clients)].reset_index(drop=True)\n",
    "\n",
    "train_dataset = MachineDataset(df_train, cat_features=CAT_FEATURES, num_features=NUM_FEATURES, target_col=TARGET[0], max_seq=MAX_SEQ, min_seq=MIN_SEQ)\n",
    "test_dataset  = MachineDataset(df_test,  cat_features=CAT_FEATURES, num_features=NUM_FEATURES, target_col=TARGET[0], max_seq=MAX_SEQ, min_seq=MIN_SEQ)\n",
    "\n",
    "# f) Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4.1) LSTM\n",
    "###############################################################################\n",
    "class LSTMRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cat_features,\n",
    "                 num_features,\n",
    "                 emb_dims,          # dict {feat: (vocab_size, emb_dim)}\n",
    "                 hidden_dim,\n",
    "                 target_size,       # |V_target|\n",
    "                 num_layers=1,\n",
    "                 dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # ── 1) Embedding for categorical features ───────────────────\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(\n",
    "                num_embeddings=emb_dims[feat][0],\n",
    "                embedding_dim=emb_dims[feat][1]\n",
    "            )\n",
    "            for feat in cat_features\n",
    "        })\n",
    "\n",
    "        # ── 2) Entry size for LSTM ──────────────────────────────────────\n",
    "        total_emb_dim = sum(emb_dims[f][1] for f in cat_features)\n",
    "        num_feat_dim  = len(num_features)                # numerical features are already in the right format\n",
    "        lstm_input_dim = total_emb_dim + num_feat_dim\n",
    "\n",
    "        # ── 3) Bidirectional LSTM ──────────────────────────────────────────\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size   = lstm_input_dim,\n",
    "            hidden_size  = hidden_dim,\n",
    "            num_layers   = num_layers,\n",
    "            batch_first  = True,\n",
    "            dropout      = dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional= True\n",
    "        )\n",
    "\n",
    "        # ── 4) Attention layer ──────────────────────────\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "        # ── 5) Classification Head ───────────────────────────────────────\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        self.fc         = nn.Linear(hidden_dim * 2, target_size)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def forward(self, x):\n",
    "        # 1. Cat embedding\n",
    "        cat_embeds = [self.embeddings[f](x[f]) for f in self.cat_features]    # list[tensor]\n",
    "        cat_emb    = torch.cat(cat_embeds, dim=-1) if cat_embeds else None   # [B, T, Σemb]\n",
    "\n",
    "        # 2. Numerical features\n",
    "        num_feats = torch.stack([x[f] for f in self.num_features],\n",
    "                                dim=-1) if self.num_features else None       # [B, T, n_num]\n",
    "\n",
    "        # 3. Concatenate embeddings and numerical features\n",
    "        if cat_emb is not None and num_feats is not None:\n",
    "            lstm_in = torch.cat([cat_emb, num_feats], dim=-1)\n",
    "        elif cat_emb is not None:\n",
    "            lstm_in = cat_emb\n",
    "        else:\n",
    "            lstm_in = num_feats                                             # [B, T, D_in]\n",
    "\n",
    "        # 4. LSTM\n",
    "        H, _ = self.lstm(lstm_in)                                           # [B, T, 2*H]\n",
    "\n",
    "        # 5. Attention : score → weight → context\n",
    "        scores  = self.attn_proj(H)                                         # [B, T, 1]\n",
    "        weights = torch.softmax(scores, dim=1)                              # [B, T, 1]\n",
    "        ctx     = (weights * H).sum(dim=1)                                  # [B, 2*H]\n",
    "\n",
    "        # 6. Norm + Dropout + FC\n",
    "        ctx = self.layer_norm(ctx)\n",
    "        out = self.dropout(ctx)\n",
    "        return self.fc(out)                                                 # [B, |V|]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4.2) GRU4Rec\n",
    "###############################################################################\n",
    "class GRU4Rec(nn.Module):\n",
    "\n",
    "    def __init__(self, cat_features, num_features, emb_dims, hidden_dim, target_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.cat_features = cat_features\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # Embeddings for categorical features\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat: nn.Embedding(\n",
    "                num_embeddings=emb_dims[feat][0],\n",
    "                embedding_dim=emb_dims[feat][1]\n",
    "            ) for feat in cat_features\n",
    "        })\n",
    "\n",
    "        # Dimensions\n",
    "        total_emb_dim = sum(emb_dims[f][1] for f in cat_features)\n",
    "        num_feat_dim = len(num_features)\n",
    "        input_dim = total_emb_dim + num_feat_dim\n",
    "\n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention projection layer\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "        # Dropout & final classifier\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, target_size)\n",
    "\n",
    "        # Optional layer normalization for the context vector\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding categorical inputs\n",
    "        cat_embeds = [self.embeddings[feat](x[feat]) for feat in self.cat_features]\n",
    "        cat_emb = torch.cat(cat_embeds, dim=-1) if cat_embeds else None\n",
    "\n",
    "        # Stacking numerical features\n",
    "        num_feats = torch.stack([x[feat] for feat in self.num_features], dim=-1) if self.num_features else None\n",
    "\n",
    "        # Combine embeddings and numerical features\n",
    "        if cat_emb is not None and num_feats is not None:\n",
    "            rnn_input = torch.cat([cat_emb, num_feats], dim=-1)\n",
    "        elif cat_emb is not None:\n",
    "            rnn_input = cat_emb\n",
    "        else:\n",
    "            rnn_input = num_feats\n",
    "\n",
    "        # Forward pass through GRU\n",
    "        H, _ = self.gru(rnn_input)  # H shape: [batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        # Attention weights\n",
    "        scores = self.attn_proj(H)              # [batch, seq_len, 1]\n",
    "        weights = torch.softmax(scores, dim=1)  # [batch, seq_len, 1]\n",
    "\n",
    "        # Weighted sum to compute the context vector\n",
    "        context = (weights * H).sum(dim=1)    # [batch, hidden_dim*2]\n",
    "        context = self.layer_norm(context)\n",
    "\n",
    "        # Dropout and final classification\n",
    "        out = self.dropout(context)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Hyperparameters (KFold CrossVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_hit3(model_class, model_kwargs,\n",
    "            df_lstm, cat_features, num_features, target_col,\n",
    "            max_seq, min_seq, k_folds, batch_size, num_epochs, lr):\n",
    "\n",
    "    clients = np.random.permutation(df_lstm[\"*****************\"].unique())\n",
    "    kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "\n",
    "    hit3_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(clients):\n",
    "\n",
    "        print(f\"\\n---- Fold: {len(hit3_scores) + 1}/{k_folds}\")\n",
    "\n",
    "        train_cli, val_cli = set(clients[train_idx]), set(clients[val_idx])\n",
    "        df_tr = df_lstm[df_lstm[\"*****************\"].isin(train_cli)]\n",
    "        df_va = df_lstm[df_lstm[\"*****************\"].isin(val_cli)]\n",
    "\n",
    "        # 2. Class Weights\n",
    "        all_labels = np.fromiter((lbl for seq in df_tr[target_col] for lbl in seq[min_seq:]), dtype=np.int64)\n",
    "        counts = np.bincount(all_labels, minlength=max(all_labels) + 1)\n",
    "        weights = 1.0 / np.log1p(np.where(counts == 0, 1, counts))\n",
    "        class_w_cpu = torch.tensor(weights, dtype=torch.float32)\n",
    "        class_w_gpu = class_w_cpu\n",
    "\n",
    "        # 3. Datasets & DataLoaders\n",
    "        tr_ds = MachineDataset(df_tr, cat_features, num_features, target_col, max_seq, min_seq)\n",
    "        va_ds = MachineDataset(df_va, cat_features, num_features, target_col, max_seq, min_seq)\n",
    "\n",
    "        samp_w = class_w_cpu[[tgt for _, tgt in tr_ds]].double()\n",
    "        train_ld = DataLoader(tr_ds, batch_size=batch_size, sampler=WeightedRandomSampler(samp_w, len(samp_w), replacement=True), collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "        val_ld = DataLoader(va_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "        # 4. Model and Training Setup\n",
    "        model = model_class(**model_kwargs)\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='max', factor=0.2, patience=1, verbose=False, min_lr=1e-6)\n",
    "        criterion = FocalLoss(alpha=class_w_gpu, gamma=2.0)\n",
    "\n",
    "        best_hit, no_improve = 0.0, 0  # early stopping variables\n",
    "\n",
    "        for _ in range(num_epochs):\n",
    "\n",
    "            # --- Training ---\n",
    "            model.train()\n",
    "            for feats, tgt in train_ld:\n",
    "                feats = {k: v for k, v in feats.items()}\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss = criterion(model(feats), tgt)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optim.step()\n",
    "\n",
    "            # --- Validation ---\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for feats, tgt in val_ld:\n",
    "                    feats = {k: v for k, v in feats.items()}\n",
    "                    top3 = model(feats).topk(3, dim=1).indices\n",
    "                    correct += (top3 == tgt.unsqueeze(1)).any(1).sum().item()\n",
    "                    total += tgt.size(0)\n",
    "\n",
    "            hit_val = correct / total\n",
    "            scheduler.step(hit_val)\n",
    "\n",
    "            if hit_val > best_hit:\n",
    "                best_hit = hit_val\n",
    "                no_improve = 0\n",
    "                print(f\"Epoch: {_ + 1}/{num_epochs}\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                print(f\"Epoch: {_ + 1}/{num_epochs} - No Improve: {no_improve}\")\n",
    "                if no_improve >= EARLY_STOPPING:\n",
    "                    break  # early stop\n",
    "\n",
    "        hit3_scores.append(best_hit)\n",
    "\n",
    "    return float(np.mean(hit3_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) Tuning avec Optuna\n",
    "###############################################################################\n",
    "# ── 1)  Utility to set emb_dims ───────────────────────────────────────\n",
    "def suggest_emb_dims(vocab_sizes):\n",
    "    out = {}\n",
    "    for feat, size in vocab_sizes.items():\n",
    "        if size < 100:\n",
    "            out[feat] = 8\n",
    "        elif size < 1000:\n",
    "            out[feat] = 32\n",
    "        else:\n",
    "            out[feat] = 64\n",
    "    return out\n",
    "\n",
    "\n",
    "# ── 2) tuning with Optuna ────────────────────────────────────────────────────\n",
    "def tune_model_with_optuna(model_tag, model_class, df_lstm, vocab_sizes, cat_features, num_features, target_vocab_size, n_trials, k_folds, num_epochs):\n",
    "    emb_dims_const = suggest_emb_dims(vocab_sizes)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        # hyperparams communs\n",
    "        max_seq    = trial.suggest_int(\"max_seq\", 20, 50)\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [32, 64, 128])\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.3, 0.6)\n",
    "        lr         = trial.suggest_loguniform(\"lr\", 1e-5, 5e-3)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "\n",
    "        model_kwargs = {\n",
    "                \"cat_features\":      cat_features,\n",
    "                \"num_features\":      num_features,\n",
    "                \"emb_dims\":          {feat: (vocab_sizes[feat], emb_dims_const[feat]) for feat in cat_features},\n",
    "                \"hidden_dim\":        hidden_dim,\n",
    "                \"target_size\":       target_vocab_size,\n",
    "                \"num_layers\":        num_layers,\n",
    "                \"dropout\":           dropout,\n",
    "            }\n",
    "\n",
    "        # Cross-Validation Hit@3\n",
    "        return cv_hit3(\n",
    "            model_class   = model_class,\n",
    "            model_kwargs  = model_kwargs,\n",
    "            df_lstm       = df_lstm,\n",
    "            cat_features  = cat_features,\n",
    "            num_features  = num_features,\n",
    "            target_col    = TARGET[0],\n",
    "            max_seq       = max_seq,\n",
    "            min_seq       = MIN_SEQ,\n",
    "            k_folds       = k_folds,\n",
    "            batch_size    = batch_size,\n",
    "            num_epochs    = num_epochs,\n",
    "            lr            = lr,\n",
    "        )\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(\"Best Hit@3 =\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_gru = tune_model_with_optuna(\n",
    "#     model_tag=\"gru\",\n",
    "#     model_class=GRU4Rec,\n",
    "#     df_lstm=df_lstm,\n",
    "#     vocab_sizes=vocab_sizes,\n",
    "#     cat_features=CAT_FEATURES,\n",
    "#     num_features=NUM_FEATURES,\n",
    "#     target_vocab_size=target_vocab_size,\n",
    "#     n_trials=20,\n",
    "#     k_folds=3,\n",
    "#     num_epochs=100,\n",
    "# )\n",
    "\n",
    "# BEST_PARAMS = study_gru.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hit@3 = *************************\n",
    "BEST_PARAMS = {'max_seq': ****, 'hidden_dim': ****,, 'num_layers': ****,, 'dropout': ****, 'lr': ****,, 'batch_size': ****}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6) Training the Best Model\n",
    "###############################################################################\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    \"max_seq\":   23,\n",
    "    \"hidden_dim\": 32,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\":   0.7848729433262691,\n",
    "    \"lr\":        0.0049013565781703,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\":  200,\n",
    "    \"patience\":  EARLY_STOPPING,\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 1) Rebuilding datasets / dataloaders with the best hyperparameters\n",
    "###############################################################################\n",
    "train_dataset_best = MachineDataset(\n",
    "    df_train,\n",
    "    cat_features = CAT_FEATURES,\n",
    "    num_features = NUM_FEATURES,\n",
    "    target_col   = TARGET[0],\n",
    "    max_seq      = BEST_PARAMS[\"max_seq\"],\n",
    "    min_seq      = MIN_SEQ,\n",
    ")\n",
    "test_dataset_best = MachineDataset(\n",
    "    df_test,\n",
    "    cat_features = CAT_FEATURES,\n",
    "    num_features = NUM_FEATURES,\n",
    "    target_col   = TARGET[0],\n",
    "    max_seq      = BEST_PARAMS[\"max_seq\"],\n",
    "    min_seq      = MIN_SEQ,\n",
    ")\n",
    "\n",
    "# Class weights (calculated on the TRAIN set only)\n",
    "all_labels = np.fromiter(\n",
    "    (lbl for seq in df_train[TARGET[0]] for lbl in seq[MIN_SEQ:]),\n",
    "    dtype=np.int64,\n",
    ")\n",
    "counts = np.bincount(all_labels, minlength=max(all_labels) + 1)\n",
    "class_weights = 1.0 / np.log1p(np.where(counts == 0, 1, counts))\n",
    "class_w_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Weighted sampler + DataLoaders\n",
    "sample_weights = class_w_tensor[[t for _, t in train_dataset_best]].double()\n",
    "train_loader_best = DataLoader(\n",
    "    train_dataset_best,\n",
    "    batch_size = BEST_PARAMS[\"batch_size\"],\n",
    "    sampler    = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True),\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 0,\n",
    "    pin_memory  = True,\n",
    ")\n",
    "test_loader_best = DataLoader(\n",
    "    test_dataset_best,\n",
    "    batch_size = BEST_PARAMS[\"batch_size\"],\n",
    "    shuffle    = False,\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 0,\n",
    "    pin_memory  = True,\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 2) Model + optimizers\n",
    "###############################################################################\n",
    "EMB_SPECS = {  # ← new name, avoids ambiguity\n",
    "    feat: (vocab_sizes[feat], emb_dims[feat])  # emb_dims = dict[int]\n",
    "    for feat in CAT_FEATURES\n",
    "}\n",
    "\n",
    "model_best = GRU4Rec(\n",
    "    cat_features = CAT_FEATURES,\n",
    "    num_features = NUM_FEATURES,\n",
    "    emb_dims     = EMB_SPECS,  # <-- use EMB_SPECS for safety\n",
    "    hidden_dim   = BEST_PARAMS[\"hidden_dim\"],\n",
    "    target_size  = target_vocab_size,\n",
    "    num_layers   = BEST_PARAMS[\"num_layers\"],\n",
    "    dropout      = BEST_PARAMS[\"dropout\"],\n",
    ")\n",
    "\n",
    "optimizer  = torch.optim.AdamW(model_best.parameters(), lr=BEST_PARAMS[\"lr\"], weight_decay=1e-4)\n",
    "scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.2,\n",
    "                                                        patience=1, verbose=False, min_lr=1e-6)\n",
    "criterion   = FocalLoss(alpha=class_w_tensor, gamma=2.0)\n",
    "\n",
    "###############################################################################\n",
    "# 3) Training loop with Early-Stopping\n",
    "###############################################################################\n",
    "best_hit, epochs_no_improve = 0.0, 0\n",
    "for epoch in range(1, BEST_PARAMS[\"n_epochs\"] + 1):\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    model_best.train()\n",
    "    for feats, tgt in train_loader_best:\n",
    "        feats = {k: v for k, v in feats.items()}\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_best(feats), tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_best.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---------- Validation (here on test_loader_best *during* training) ----------\n",
    "    model_best.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for feats, tgt in test_loader_best:\n",
    "            feats = {k: v for k, v in feats.items()}\n",
    "            top3 = model_best(feats).topk(3, dim=1).indices\n",
    "            correct += (top3 == tgt.unsqueeze(1)).any(1).sum().item()\n",
    "            total   += tgt.size(0)\n",
    "\n",
    "    hit3 = correct / total\n",
    "    scheduler.step(hit3)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Hit@3 = {hit3:.4f}\")\n",
    "\n",
    "    # ---------- Early-Stopping ----------\n",
    "    if hit3 > best_hit:\n",
    "        best_hit = hit3\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model_best.state_dict(), \"gru4rec_best.pt\")   # save model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Epoch {epoch:03d} | No Improvement: {epochs_no_improve}/{BEST_PARAMS['patience']}\")\n",
    "        if epochs_no_improve >= BEST_PARAMS[\"patience\"]:\n",
    "            print(\"Early stop\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Recreate test_loader_best if necessary\n",
    "train_dataset_best = MachineDataset(\n",
    "    df_train, cat_features=CAT_FEATURES, num_features=NUM_FEATURES,\n",
    "    target_col=TARGET[0], max_seq=BEST_PARAMS[\"max_seq\"], min_seq=MIN_SEQ\n",
    ")\n",
    "test_dataset_best = MachineDataset(\n",
    "    df_test, cat_features=CAT_FEATURES, num_features=NUM_FEATURES,\n",
    "    target_col=TARGET[0], max_seq=BEST_PARAMS[\"max_seq\"], min_seq=MIN_SEQ\n",
    ")\n",
    "\n",
    "# Compute class weights on df_train\n",
    "all_labels = np.fromiter((lbl for seq in df_train[TARGET[0]] for lbl in seq[MIN_SEQ:]), dtype=np.int64)\n",
    "counts = np.bincount(all_labels, minlength=max(all_labels) + 1)\n",
    "class_weights = 1.0 / np.log1p(np.where(counts == 0, 1, counts))\n",
    "class_w_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# WeightedRandomSampler for training (not required for evaluation, but shows full pipeline)\n",
    "sample_weights = class_w_tensor[[t for _, t in train_dataset_best]].double()\n",
    "train_loader_best = torch.utils.data.DataLoader(\n",
    "    train_dataset_best, batch_size=BEST_PARAMS[\"batch_size\"],\n",
    "    sampler=WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True),\n",
    "    collate_fn=collate_fn, num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader_best = torch.utils.data.DataLoader(\n",
    "    test_dataset_best, batch_size=BEST_PARAMS[\"batch_size\"],\n",
    "    shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# 2) Load/reinstantiate the model and switch to eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_best = GRU4Rec(\n",
    "    cat_features=CAT_FEATURES,\n",
    "    num_features=NUM_FEATURES,\n",
    "    emb_dims={feat: (vocab_sizes[feat], emb_dims[feat]) for feat in CAT_FEATURES},\n",
    "    hidden_dim=BEST_PARAMS[\"hidden_dim\"],\n",
    "    target_size=target_vocab_size,\n",
    "    num_layers=BEST_PARAMS[\"num_layers\"],\n",
    "    dropout=BEST_PARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_best.load_state_dict(torch.load(\"gru4rec_best.pt\", map_location=device))\n",
    "model_best.eval()\n",
    "\n",
    "# 3) Retrieve logits and targets on the test set\n",
    "all_logits = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for feats, tgt in test_loader_best:\n",
    "        feats = {k: v.to(device) for k, v in feats.items()}\n",
    "        tgt = tgt.to(device)\n",
    "        output = model_best(feats)  # shape [batch_size, num_classes]\n",
    "        all_logits.append(output.cpu())\n",
    "        all_targets.append(tgt.cpu())\n",
    "\n",
    "logits = torch.cat(all_logits, dim=0).numpy()\n",
    "targets = torch.cat(all_targets, dim=0).numpy()\n",
    "num_samples = targets.shape[0]\n",
    "\n",
    "# 4) Compute Hit@k for k = 1..5\n",
    "max_k = 5\n",
    "hit_at_k = []\n",
    "for k in range(1, max_k + 1):\n",
    "    topk_preds = np.argsort(logits, axis=1)[:, -k:]\n",
    "    hits = [1 if targets[i] in topk_preds[i] else 0 for i in range(num_samples)]\n",
    "    hit_at_k.append(np.mean(hits))\n",
    "\n",
    "# 5) Plot Hit@k\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, max_k + 1), hit_at_k, marker='o')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Hit@k\")\n",
    "plt.title(\"Hit@k for k = 1 to 5\")\n",
    "plt.xticks(range(1, max_k + 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6) Classification report and per-class metrics (top-1)\n",
    "top1_preds = np.argmax(logits, axis=1)\n",
    "\n",
    "# Reverse LabelToIdx to retrieve class names\n",
    "inv_LabelToIdx = {v: k for k, v in LabelToIdx['***********'].items()}\n",
    "target_names = [inv_LabelToIdx[i] for i in sorted(inv_LabelToIdx.keys())]\n",
    "y_true_str = [inv_LabelToIdx[t] for t in targets]\n",
    "y_pred_str = [inv_LabelToIdx[p] for p in top1_preds]\n",
    "\n",
    "report = classification_report(y_true_str, y_pred_str, labels=target_names, output_dict=True)\n",
    "print(\"Classification Report (per class):\")\n",
    "for cls in target_names:\n",
    "    pr = report[cls]['precision']\n",
    "    rc = report[cls]['recall']\n",
    "    f1 = report[cls]['f1-score']\n",
    "    sup = report[cls]['support']\n",
    "    print(f\"{cls:<10} | Precision = {pr:.3f} | Recall = {rc:.3f} | F1 = {f1:.3f} | Support = {sup}\")\n",
    "\n",
    "# 7) Plot F1-score per class\n",
    "class_f1 = [report[cls]['f1-score'] for cls in target_names]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(len(target_names)), class_f1)\n",
    "plt.xlabel(\"Class (************)\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1-score per class\")\n",
    "plt.xticks(range(len(target_names)), target_names, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7) Training the Final Model on the Entire Dataset\n",
    "###############################################################################\n",
    "\n",
    "# a) Re‑create the full dataset\n",
    "full_dataset = MachineDataset(\n",
    "    df_lstm,               # Use the entire dataset\n",
    "    cat_features=CAT_FEATURES,\n",
    "    num_features=NUM_FEATURES,\n",
    "    target_col=TARGET[0],\n",
    "    max_seq=BEST_PARAMS[\"max_seq\"],\n",
    "    min_seq=MIN_SEQ\n",
    ")\n",
    "\n",
    "# b) Compute class weights on the full set\n",
    "all_labels = np.fromiter(\n",
    "    (lbl for seq in df_lstm[TARGET[0]] for lbl in seq[MIN_SEQ:]),\n",
    "    dtype=np.int64\n",
    ")\n",
    "counts = np.bincount(all_labels, minlength=max(all_labels) + 1)\n",
    "class_weights = 1.0 / np.log1p(np.where(counts == 0, 1, counts))\n",
    "class_w_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# c) Create the DataLoader with a WeightedSampler\n",
    "sample_weights = class_w_tensor[[t for _, t in full_dataset]].double()\n",
    "full_loader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=BEST_PARAMS[\"batch_size\"],\n",
    "    sampler=WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True),\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# d) Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_model = GRU4Rec(\n",
    "    cat_features=CAT_FEATURES,\n",
    "    num_features=NUM_FEATURES,\n",
    "    emb_dims={feat: (vocab_sizes[feat], emb_dims[feat]) for feat in CAT_FEATURES},\n",
    "    hidden_dim=BEST_PARAMS[\"hidden_dim\"],\n",
    "    target_size=target_vocab_size,\n",
    "    num_layers=BEST_PARAMS[\"num_layers\"],\n",
    "    dropout=BEST_PARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "# e) Optimizer and Loss\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=BEST_PARAMS[\"lr\"], weight_decay=1e-4)\n",
    "criterion = FocalLoss(alpha=class_w_tensor.to(device), gamma=2.0)\n",
    "\n",
    "# f) Full training loop\n",
    "num_epochs = *********   # Number of epochs observed during the best run\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    final_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for feats, tgt in full_loader:\n",
    "        feats = {k: v.to(device) for k, v in feats.items()}\n",
    "        tgt   = tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(feats)\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(full_loader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# g) Inference + Client-Centered DataFrame (Top-k)\n",
    "###############################################################################\n",
    "k = 5  # ← Desired Top-k\n",
    "\n",
    "# 1. Dataset that also returns *****************\n",
    "class InferDataset(Dataset):\n",
    "    def __init__(self, df_lstm, *args, **kwargs):\n",
    "        self.base = MachineDataset(df_lstm, *args, **kwargs)\n",
    "        self.ids  = np.repeat(df_lstm['*****************'].values,\n",
    "                              [len(seq) - 1 for seq in df_lstm[TARGET[0]]])\n",
    "\n",
    "    def __len__(self):              \n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[idx]\n",
    "        return x, y, self.ids[idx]  # ← (features, target, id_client)\n",
    "\n",
    "infer_ds = InferDataset(\n",
    "    df_lstm,\n",
    "    cat_features = CAT_FEATURES,\n",
    "    num_features = NUM_FEATURES,\n",
    "    target_col   = TARGET[0],\n",
    "    max_seq      = BEST_PARAMS[\"max_seq\"],\n",
    "    min_seq      = MIN_SEQ,\n",
    ")\n",
    "\n",
    "infer_loader = DataLoader(\n",
    "    infer_ds, batch_size=256, shuffle=False, collate_fn=lambda batch:\n",
    "    (\n",
    "        {f: pad_sequence([b[0][f] for b in batch],\n",
    "                         batch_first=True,\n",
    "                         padding_value=0 if batch[0][0][f].dtype==torch.long else 0.)\n",
    "         for f in batch[0][0]},\n",
    "        torch.stack([b[1] for b in batch]),\n",
    "        [b[2] for b in batch]\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Top-k Predictions\n",
    "inv_simma = {v: k for k, v in LabelToIdx['*****************'].items()}\n",
    "topk_by_cli = {}  # ***************** -> [idx1, idx2, … idxk]\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for feats, _, cli_ids in infer_loader:\n",
    "        feats = {k: v.to(device) for k, v in feats.items()}\n",
    "        logits = final_model(feats)             # [B, |V|]\n",
    "        topk   = logits.argsort(dim=1)[:, -k:]  # [B, k] (encoded)\n",
    "\n",
    "        for cid, preds in zip(cli_ids, topk.cpu().numpy()):\n",
    "            topk_by_cli[cid] = preds  # keep only the last sequence per client\n",
    "\n",
    "# 3. Metadata: Name, Department, Family ↔ CodeSimma\n",
    "meta = pd.read_csv(\n",
    "    PATH_CSV,\n",
    "    usecols=['*****************', '*****************', '*****************',\n",
    "             '*****************', '*****************']\n",
    ").drop_duplicates()\n",
    "\n",
    "map_fam = meta[['*****************', '*****************']].drop_duplicates()\n",
    "map_fam = dict(map_fam.values)  # ***************** -> *****************\n",
    "\n",
    "client_info = meta[['*****************', '*****************',\n",
    "                    '*****************']].drop_duplicates()\n",
    "\n",
    "# 4. Build the final DataFrame\n",
    "records = []\n",
    "for cid, preds in topk_by_cli.items():\n",
    "    row = {\n",
    "        '*****************': cid,\n",
    "        '*****************': client_info.loc[client_info.IdClient==cid,\n",
    "                                      '*****************'].iat[0],\n",
    "        '*****************': client_info.loc[client_info.IdClient==cid,\n",
    "                                       '*****************'].iat[0],\n",
    "    }\n",
    "    # Top-k\n",
    "    for rank, idx in enumerate(preds[::-1], start=1):  # from best to least good\n",
    "        code = inv_simma[idx]\n",
    "        fam  = map_fam.get(code, 'NA')\n",
    "        row[f'pred_{rank}_*****************'] = code\n",
    "        row[f'pred_{rank}_*****************'] = fam\n",
    "    records.append(row)\n",
    "\n",
    "pred_df = pd.DataFrame(records).sort_values('*****************').reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
